{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T17:05:27.682281Z","iopub.status.busy":"2024-04-09T17:05:27.681503Z","iopub.status.idle":"2024-04-09T17:05:27.686476Z","shell.execute_reply":"2024-04-09T17:05:27.685590Z","shell.execute_reply.started":"2024-04-09T17:05:27.682221Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import time"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data = pd.read_csv('/kaggle/input/1300-towards-datascience-medium-articles-dataset/medium.csv')\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["token = HUGGINGFACE_TOKEN"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","#tokenizer and model initialization\n","tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\",token=token, device_map='cuda')\n","model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", token=token, device_map='cuda')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","for n,i in enumerate(data['Text']):\n","    start = time.time()\n","    try:\n","        prompt_template = \"Generate five questions based on the following statement: '{}'. Question:\"\n","        input_string = i\n","        prompt = prompt_template.format(input_string)\n","        input_ids = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n","\n","        # questions generation by Gemma\n","        outputs = model.generate(**input_ids, max_new_tokens=1000)\n","\n","        input_string = tokenizer.decode(outputs[0])\n","        question_index = input_string.find(\"Question:\")\n","        if question_index != -1:  \n","        #question save to txt\n","            cropped_string = input_string[question_index:]\n","            print(cropped_string)\n","            output_dir = \"/kaggle/working/\" \n","\n","            output_file = output_dir + f\"/question{n}.txt\"\n","            with open(output_file, 'w') as outfile:\n","                outfile.write(cropped_string)\n","        else:\n","            print(\"bad\")\n","        end = time.time()\n","        print(end - start)\n","    except:\n","        pass"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4564591,"sourceId":7796657,"sourceType":"datasetVersion"}],"dockerImageVersionId":30674,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
